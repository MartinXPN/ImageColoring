{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import threading\n",
    "from PIL import Image\n",
    "from skimage import io, color\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import *\n",
    "from keras.layers import *\n",
    "from keras.layers.core import Lambda\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from slice import Slice\n",
    "from colorizer import create_colorizer\n",
    "from discriminator import create_discriminator\n",
    "\n",
    "\n",
    "# We need only the conv part of VGG\n",
    "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "image_mean = (103.939, 116.779, 123.68)\n",
    "greyscale_image_mean = np.mean(image_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_trainable(layers, is_trainable):\n",
    "    for l in layers:\n",
    "        l.trainable = is_trainable\n",
    "        \n",
    "def create_GAN(generator, discriminator):\n",
    "    generator_trainable_layers = [layer for layer in generator.layers if layer.trainable]\n",
    "    discriminator_trainable_layers = [layer for layer in discriminator.layers if layer.trainable]\n",
    "\n",
    "    network_input = Input(shape=(224, 224, 3))\n",
    "    generator_output = generator(network_input)\n",
    "\n",
    "    lightness = Slice(slice(0, 1), axis=3) (network_input)\n",
    "    lab = concatenate([lightness, generator_output])\n",
    "    network_output = discriminator(lab)\n",
    "\n",
    "    GAN = Model(network_input, network_output)\n",
    "    return GAN, generator_trainable_layers, discriminator_trainable_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "colorizer, vgg = create_colorizer()\n",
    "discriminator = create_discriminator()\n",
    "gan, generator_trainable_layers, discriminator_trainable_layers = create_GAN(colorizer, discriminator)\n",
    "\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s\n",
      "(224, 224, 2)\n"
     ]
    }
   ],
   "source": [
    "img = load_img('./dog.jpg', target_size=(224, 224))\n",
    "x = img_to_array(img)\n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "# res_first = first.predict([x], 1, verbose=1)\n",
    "res_second = colorizer.predict([x], 1, verbose=1)\n",
    "\n",
    "# print(res_first[0].shape)\n",
    "print(res_second[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import threading\n",
    "\n",
    "class BaseBatchGenerator:\n",
    "    \n",
    "    def __init__(self, image_paths, batch_size, image_height, image_width, shuffle = True):\n",
    "        self.image_paths = image_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "        self.lock = threading.Lock()\n",
    "        self.index = 0\n",
    "        if shuffle:\n",
    "            random.shuffle(self.image_paths)\n",
    "\n",
    "    def next(self):\n",
    "        with self.lock:\n",
    "            batch_features = []\n",
    "            batch_labels   = []\n",
    "\n",
    "            for i in range(self.batch_size):\n",
    "                path = self.image_paths[self.index]\n",
    "                x, y = generate_one(path)\n",
    "\n",
    "                self.index += 1\n",
    "                if self.index >= len(self.image_paths):\n",
    "                    self.index = 0\n",
    "            \n",
    "            return np.array(batch_features), np.array(batch_labels)\n",
    "    \n",
    "    def generate_one(path):\n",
    "        raise NotImplementedError(\"Please Implement this method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "\n",
    "    def __init__(self, image_paths, batch_size, image_height, image_width, shuffle = True):\n",
    "        self.image_paths = image_paths\n",
    "        self.batch_size = batch_size\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "        self.generate_for_discriminator = True\n",
    "        self.lock = threading.Lock()\n",
    "        self.index = 0\n",
    "        if shuffle:\n",
    "            random.shuffle(self.image_paths)\n",
    "\n",
    "    def next(self, is_real):\n",
    "        with self.lock:\n",
    "            batch_features = np.zeros((self.batch_size, self.image_height, self.image_width, 3))\n",
    "            batch_labels   = np.zeros((self.batch_size))\n",
    "\n",
    "            for i in range(self.batch_size):\n",
    "                path = self.image_paths[self.index]\n",
    "                \n",
    "                if self.generate_for_discriminator:\n",
    "                    image_gray = load_img(path, grayscale=True, target_size=(self.image_height, self.image_width))\n",
    "                    batch_features[i,:,:,0] = image_gray - greyscale_image_mean\n",
    "                    batch_features[i,:,:,1] = image_gray - greyscale_image_mean\n",
    "                    batch_features[i,:,:,2] = image_gray - greyscale_image_mean\n",
    "                    batch_labels[i] = is_real\n",
    "                else:\n",
    "                    image = io.imread(path)\n",
    "                    image = resize(image, output_shape=(self.image_height, self.image_width, 3))\n",
    "                    image_gray = load_img(path, grayscale=True, target_size=(self.image_height, self.image_width))\n",
    "                    batch_features[i] = color.rgb2lab(image)\n",
    "                    batch_features[i,:,:,0] = image_gray - greyscale_image_mean\n",
    "                    batch_labels[i] = is_real\n",
    "\n",
    "                self.index += 1\n",
    "                if self.index >= len(self.image_paths):\n",
    "                    self.index = 0\n",
    "            \n",
    "            return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 82783\n",
      "Validation: 40504\n",
      "Test: 40775\n"
     ]
    }
   ],
   "source": [
    "# Define paths for train / validation / test datasets\n",
    "train_root = '../coco/train2014/'\n",
    "valid_root = '../coco/val2014/'\n",
    "test_root = '../coco/test2014/'\n",
    "\n",
    "train_image_paths = [train_root + item for item in os.listdir(train_root) if item.endswith('.jpg') ]\n",
    "valid_image_paths = [valid_root + item for item in os.listdir(valid_root) if item.endswith('.jpg') ]\n",
    "test_image_paths = [test_root + item for item in os.listdir(test_root) if item.endswith('.jpg') ]\n",
    "\n",
    "print( 'Train:', len(train_image_paths) )\n",
    "print( 'Validation:', len(valid_image_paths) )\n",
    "print( 'Test:', len(test_image_paths) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2237 steps per one train epoch for one complete cycle\n",
      "There are 1094 steps per one validation epoch for one complete cycle\n"
     ]
    }
   ],
   "source": [
    "batch_size = 37\n",
    "train_data_generator = BatchGenerator(image_paths=train_image_paths, batch_size=batch_size, image_height=224, image_width=224)\n",
    "valid_data_generator = BatchGenerator(image_paths=valid_image_paths, batch_size=batch_size, image_height=224, image_width=224)\n",
    "\n",
    "print('There are', len(train_image_paths) / batch_size, 'steps per one train epoch for one complete cycle')\n",
    "print('There are', len(valid_image_paths) / batch_size, 'steps per one validation epoch for one complete cycle')\n",
    "\n",
    "checkpoint_path=\"../checkpoints/model-{epoch:03d}-{loss:.4f}.model\"\n",
    "model_saver = ModelCheckpoint(filepath=checkpoint_path, monitor='loss', save_best_only=True, mode='min')\n",
    "# tensorboard = TensorBoard(log_dir='/tmp/coloring/')\n",
    "\n",
    "\n",
    "epochs = 77\n",
    "steps_per_epoch = 500\n",
    "validation_steps = 33\n",
    "# callbacks = [tensorboard, model_saver]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch 0:   0%|          | 1/1500 [00:27<11:36:49, 27.89s/it, loss=0.729]/home/martin/.miniconda2/lib/python2.7/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "Train batch 370:  25%|██▍       | 371/1500 [47:26<2:23:43,  7.64s/it, loss=15.9]    "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-58fe29c7d659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Display progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/martin/.miniconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1563\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/martin/.miniconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/martin/.miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/martin/.miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/martin/.miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/martin/.miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/martin/.miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    steps = [0, 1, 2] * steps_per_epoch\n",
    "    t = trange( len(steps) )\n",
    "    for i in t:\n",
    "        step = steps[i]\n",
    "        \n",
    "        # Make discriminator better on predicting real images\n",
    "        if step == 0:\n",
    "            train_data_generator.generate_for_discriminator = True\n",
    "            net = discriminator                                   # Propagate the data only through discriminator\n",
    "            make_trainable(discriminator_trainable_layers, True)  # make discriminator layers trainable\n",
    "            make_trainable(generator_trainable_layers, False)     # we don't touch the generator\n",
    "            batch = train_data_generator.next(is_real=True)       # Images are real\n",
    "                \n",
    "        # Make discriminator better on detecting fake images\n",
    "        elif step == 1:\n",
    "            train_data_generator.generate_for_discriminator = False\n",
    "            net = gan                                             # Propagate the data though the whole gan\n",
    "            make_trainable(discriminator_trainable_layers, True)  # Train the discriminator\n",
    "            make_trainable(generator_trainable_layers, False)     # But don't touch the generator as it's not his turn\n",
    "            batch = train_data_generator.next(is_real=False)      # Images are fake\n",
    "        \n",
    "        # Make colorizer better in fooling the discriminator\n",
    "        else:\n",
    "            train_data_generator.generate_for_discriminator = False\n",
    "            net = gan                                             # Propagate the data through the whole gan\n",
    "            make_trainable(discriminator_trainable_layers, False) # Don't train the discriminator when fooling it\n",
    "            make_trainable(generator_trainable_layers, True)      # Train only the generator\n",
    "            batch = train_data_generator.next(is_real=True)       # Lets fool the discriminator\n",
    "\n",
    "        \n",
    "        x, y = batch\n",
    "        loss = net.train_on_batch(x, y)\n",
    "        \n",
    "        # Display progress\n",
    "        t.set_description('Train batch %i' % i)\n",
    "        t.set_postfix(loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('../checkpoints/model-000-730.5353.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = './puma.jpg' # random.choice(test_image_paths)\n",
    "\n",
    "rgb = io.imread(path)\n",
    "rgb = resize(rgb, output_shape=(224, 224, 3))\n",
    "image_gray = load_img(path, grayscale=True, target_size=(224, 224))\n",
    "lab = color.rgb2lab(rgb)\n",
    "\n",
    "inp = np.zeros((224, 224, 3))\n",
    "inp[:,:,0] = image_gray - greyscale_image_mean\n",
    "inp[:,:,1] = image_gray - greyscale_image_mean\n",
    "inp[:,:,2] = image_gray - greyscale_image_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out = model.predict(np.array([inp]), batch_size=1)[0]\n",
    "\n",
    "img = np.zeros((224, 224, 3))\n",
    "img[:,:,0] = lab[...,0]\n",
    "img[:,:,1:] = out\n",
    "\n",
    "plt.imshow(color.lab2rgb(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
